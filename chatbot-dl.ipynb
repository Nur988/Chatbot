{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nimport numpy as np\n#nltk.download('punkt')\nfrom nltk.stem.porter import PorterStemmer\nfrom torch.utils.data import Dataset,DataLoader\n\n\nstemmer=PorterStemmer()\n\ndef tokenize(sentence):\n    return nltk.word_tokenize(sentence)\n\ndef stem(word):\n    return stemmer.stem(word.lower())\n\n\ndef bag_of_words(tokenized_sentence,all_words):\n    tokenized_sentence=[stem(w) for w in tokenized_sentence]\n    bag=np.zeros(len(all_words),dtype=np.float32)\n\n    for idx,w in enumerate(all_words):\n        if  w in tokenized_sentence:\n            bag[idx]=1.0\n    return bag\n\n\na=['a','q','q','w']    \nwords=['a','b','q','r','t','w']\nbag=bag_of_words(a,words)\nprint(bag)","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[1. 0. 1. 0. 0. 1.]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n \n\nclass NeuralNet(nn.Module):\n    def __init__(self,input_size,hidden_size,num_classes):\n         super(NeuralNet,self).__init__()\n         self.l1=nn.Linear(input_size,hidden_size)\n         self.l2=nn.Linear(hidden_size,hidden_size)\n         self.l3=nn.Linear(hidden_size,num_classes)\n         self.relu=nn.ReLU()\n\n    def forward(self,x):\n        output=self.relu(self.l1(x))\n        output=self.relu(self.l2(output))\n        output=self.l3(output)     \n        return output","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ChatDataset(Dataset):\n\n    def __init__(self):\n        self.n_samples = len(X_train)\n        self.x_data = X_train\n        self.y_data = y_train\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nimport json\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#from nltk_utils import bag_of_words, tokenize, stem\n#from model import NeuralNet\n\nwith open('../input/chtabot/intents.json', 'r') as f:\n    intents = json.load(f)\n\nall_words = []\ntags = []\nxy = []\n# loop through each sentence in our intents patterns\nfor intent in intents['intents']:\n    tag = intent['tag']\n    # add to tag list\n    tags.append(tag)\n    for pattern in intent['patterns']:\n        # tokenize each word in the sentence\n        w = tokenize(pattern)\n        # add to our words list\n        all_words.extend(w)\n        # add to xy pair\n        xy.append((w, tag))\n\n# stem and lower each word\nignore_words = ['?', '.', '!']\nall_words = [stem(w) for w in all_words if w not in ignore_words]\n# remove duplicates and sort\nall_words = sorted(set(all_words))\ntags = sorted(set(tags))\n\nprint(len(xy), \"patterns\")\nprint(len(tags), \"tags:\", tags)\nprint(len(all_words), \"unique stemmed words:\", all_words)\n\n# create training data\nX_train = []\ny_train = []\nfor (pattern_sentence, tag) in xy:\n    # X: bag of words for each pattern_sentence\n    bag = bag_of_words(pattern_sentence, all_words)\n    X_train.append(bag)\n    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n    label = tags.index(tag)\n    y_train.append(label)\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"60 patterns\n20 tags: ['Company', 'Weather', 'contact', 'delivery', 'funny', 'goodbye', 'greeting', 'job', 'location', 'missing_id', 'noanswer', 'opentoday', 'options', 'order_components', 'order_tracking', 'payments', 'search_department', 'service', 'thanks', 'unknown']\n97 unique stemmed words: [\"'s\", '345a23', '431b67', '561a24', '562b78', 'a', 'accept', 'ani', 'antopoli', 'anyon', 'are', 'assist', 'be', 'bye', 'can', 'card', 'cash', 'compani', 'compon', 'compris', 'contact', 'could', 'credit', 'date', 'day', 'deliveri', 'depart', 'do', 'doe', 'find', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'hour', 'how', 'i', 'id', 'in', 'internship', 'is', 'job', 'joke', 'kind', 'know', 'later', 'list', 'locat', 'long', 'lot', 'mastercard', 'me', 'my', 'need', 'of', 'offic', 'onli', 'open', 'order', 'pay', 'paypal', 'problem', 'provid', 'see', 'sell', 'servic', 'ship', 'someth', 'take', 'talk', 'tell', 'thank', 'that', 'the', 'there', 'thi', 'to', 'today', 'touch', 'track', 'vacanc', 'variou', 'want', 'weather', 'what', 'when', 'where', 'which', 'who', 'with', 'you', 'your']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyper-parameters \nnum_epochs = 1000\nbatch_size = 8\nlearning_rate = 0.001\ninput_size = len(X_train[0])\nhidden_size = 1024\noutput_size = len(tags)\nprint(input_size, output_size)\n","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"97 20\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = ChatDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=0)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Train the model\nfor epoch in range(num_epochs):\n    for (words, labels) in train_loader:\n        words = words.to(device)\n        labels = labels.to(dtype=torch.long).to(device)\n        \n        # Forward pass\n        outputs = model(words)\n        # if y would be one-hot, we must apply\n        # labels = torch.max(labels, 1)[1]\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    if (epoch+1) % 100 == 0:\n        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n\nprint(f'final loss: {loss.item():.4f}')\n\n","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch [100/1000], Loss: 0.0001\nEpoch [200/1000], Loss: 0.0000\nEpoch [300/1000], Loss: 0.0000\nEpoch [400/1000], Loss: 0.0000\nEpoch [500/1000], Loss: 0.0000\nEpoch [600/1000], Loss: 0.0000\nEpoch [700/1000], Loss: 0.0000\nEpoch [800/1000], Loss: 0.0000\nEpoch [900/1000], Loss: 0.0000\nEpoch [1000/1000], Loss: 0.0000\nfinal loss: 0.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"data = {\n\"model_state\": model.state_dict(),\n\"input_size\": input_size,\n\"hidden_size\": hidden_size,\n\"output_size\": output_size,\n\"all_words\": all_words,\n\"tags\": tags\n}\n\nFILE = \"data.pth\"\ntorch.save(data, FILE)\n\nprint(f'training complete. file saved to {FILE}')","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"training complete. file saved to data.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport json\n\nimport torch\n\n#from model import NeuralNet\n#from nltk_utils import bag_of_words, tokenize\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nwith open('../input/chtabot/intents.json', 'r') as json_data:\n    intents = json.load(json_data)\n\nFILE = \"data.pth\"\ndata = torch.load(FILE)\n\ninput_size = data[\"input_size\"]\nhidden_size = data[\"hidden_size\"]\noutput_size = data[\"output_size\"]\nall_words = data['all_words']\ntags = data['tags']\nmodel_state = data[\"model_state\"]\n\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\nmodel.load_state_dict(model_state)\nmodel.eval()\n\nbot_name = \"Sam\"\nprint(\"Let's chat! (type 'quit' to exit)\")\nwhile True:\n    # sentence = \"do you use credit cards?\"\n    sentence = input(\"You: \")\n    if sentence == \"quit\":\n        break\n\n    sentence = tokenize(sentence)\n    X = bag_of_words(sentence, all_words)\n    X = X.reshape(1, X.shape[0])\n    X = torch.from_numpy(X).to(device)\n\n    output = model(X)\n    _, predicted = torch.max(output, dim=1)\n\n    tag = tags[predicted.item()]\n\n    probs = torch.softmax(output, dim=1)\n    prob = probs[0][predicted.item()]\n    if prob.item() > 0.75:\n        for intent in intents['intents']:\n            if tag == intent[\"tag\"]:\n                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n    else:\n        print(f\"{bot_name}: I do not understand...\")","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Let's chat! (type 'quit' to exit)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  who are you\n"},{"name":"stdout","text":"Sam: Antopolis is an innovation driven enterprise that provides a range of tech and marketing services tailored to EMPOWER CHANGEMAKERS.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  you who\n"},{"name":"stdout","text":"Sam: Antopolis is an innovation driven enterprise that provides a range of tech and marketing services tailored to EMPOWER CHANGEMAKERS.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what type of work do you do for people\n"},{"name":"stdout","text":"Sam: We provide a range of tech and marketing services\n","output_type":"stream"}]}]}